{
  "models": [
    {
      "id": "qwen2.5-3b",
      "name": "Qwen2.5 3B Instruct",
      "repo": "Qwen/Qwen2.5-3B-Instruct-GGUF",
      "filename_template": "qwen2.5-3b-instruct-{quant}.gguf",
      "min_vram_gb": 3.0,
      "description": "Fast multilingual model, excellent for coding and general chat",
      "tags": ["fast", "multilingual", "coding"]
    },
    {
      "id": "phi-3-mini",
      "name": "Phi-3 Mini 4K Instruct",
      "repo": "microsoft/Phi-3-mini-4k-instruct-gguf",
      "filename_template": "Phi-3-mini-4k-instruct.Q4_K_M.gguf",
      "min_vram_gb": 3.0,
      "description": "Microsoft's efficient small model, great balance of quality and speed (only Q4 available)",
      "tags": ["balanced", "microsoft"],
      "default_quant": "Q4_K_M"
    },
    {
      "id": "llama-3.2-3b",
      "name": "Llama 3.2 3B Instruct",
      "repo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "filename_template": "Llama-3.2-3B-Instruct-{quant}.gguf",
      "min_vram_gb": 3.0,
      "description": "Meta's latest small model, modern architecture, excellent instruction following",
      "tags": ["modern", "meta", "instruction-tuned"]
    },
    {
      "id": "gemma-2-2b",
      "name": "Gemma 2 2B Instruct",
      "repo": "bartowski/gemma-2-2b-it-GGUF",
      "filename_template": "gemma-2-2b-it-{quant}.gguf",
      "min_vram_gb": 2.5,
      "description": "Google's lightweight model, very fast, good for quick tasks",
      "tags": ["lightweight", "fast", "google"]
    },
    {
      "id": "qwen2.5-7b",
      "name": "Qwen2.5 7B Instruct",
      "repo": "Qwen/Qwen2.5-7B-Instruct-GGUF",
      "filename_template": "qwen2.5-7b-instruct-{quant}.gguf",
      "min_vram_gb": 5.0,
      "description": "High quality 7B model, excellent reasoning and multilingual capabilities",
      "tags": ["high-quality", "multilingual", "reasoning"]
    },
    {
      "id": "mistral-7b",
      "name": "Mistral 7B Instruct v0.3",
      "repo": "TheBloke/Mistral-7B-Instruct-v0.3-GGUF",
      "filename_template": "mistral-7b-instruct-v0.3.{quant}.gguf",
      "min_vram_gb": 5.0,
      "description": "Strong reasoning model, good at following complex instructions",
      "tags": ["reasoning", "instruction-tuned"]
    },
    {
      "id": "deepseek-coder-6.7b",
      "name": "DeepSeek Coder 6.7B",
      "repo": "TheBloke/deepseek-coder-6.7B-instruct-GGUF",
      "filename_template": "deepseek-coder-6.7b-instruct.{quant}.gguf",
      "min_vram_gb": 4.5,
      "description": "Specialized for code generation and technical tasks",
      "tags": ["coding", "technical", "specialized"]
    }
  ],
  "quantizations": {
    "Q2_K": {"description": "Smallest, fastest, lowest quality"},
    "Q3_K_S": {"description": "Small and fast, decent quality"},
    "Q3_K_M": {"description": "Balanced 3-bit quantization"},
    "Q3_K_L": {"description": "Higher quality 3-bit"},
    "Q4_0": {"description": "Legacy 4-bit format"},
    "Q4_K_S": {"description": "Small 4-bit, good speed"},
    "Q4_K_M": {"description": "Recommended default, balanced quality/size"},
    "Q4_K_L": {"description": "High quality 4-bit"},
    "Q5_K_S": {"description": "Higher quality, larger size"},
    "Q5_K_M": {"description": "Near-lossless quality"},
    "Q6_K": {"description": "Very high quality"},
    "Q8_0": {"description": "Best quality, largest 8-bit"}
  }
}
